The model is saved and derived from the training file, the description column is preprocessed and cleaned. In the initial preprocessing steps, empty rows are removed, SQL script checks whether the text column contains any links. Furthermore, words with a frequency of less than three are filtered out. The rows that are null are removed as well while doing this all the words are converted into lowercase. I have used the custom stopwords to remove the stopwords from the dataset, as importing the stopwords package and cleaning is taking a lot of time. The rationale behind filtering out words with a frequency of less than three is to improve model accuracy. Including words that occur very infrequently can negatively impact model performance. For instance, if a word is present in the training data but does not appear in the test dataset, the model might not generalize well and could lead to inaccurate predictions. Therefore, by removing such infrequent words, the main aim is to enhance the model's ability to make more reliable predictions.
The next part is training the model and converting the description column and genre column into numerical format. For converting the genre in numerical, we are creating the matrix with the size of the data frame shape and a number of distinct genres present. The next step is creating a matrix with 1s and 0s according to the lookup table. Using text vectorization, convert text into numbers. The neural network model is constructed using tensorflow and keras. In text vectorization, the vocabulary size is set to 15000 words. The model has 64 dense layers where it has relu activation and 28 layers using softmax activation layer for multi-class classification of genres. The embedding layer is used for mapping vocabulary tokens. Adam optimizer is used for model training.
The model is evaluated on test data and the model is saved using Keras. Here pickle is used for storing a lookup table for predicting the genres. Once we pass the input file to the model it predicts the top 3 genres suitable for that description.
