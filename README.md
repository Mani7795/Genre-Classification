# Classify Design
The model is saved and derived from the training file, the description column is preprocessed and cleaned. In the initial preprocessing steps, empty rows are removed, SQL script checks whether the text column contains any links. Furthermore, words with a frequency of less than three are filtered out. The rows that are null are removed as well while doing this all the words are converted into lowercase. I have used the custom stopwords to remove the stopwords from the dataset, as importing the stopwords package and cleaning is taking a lot of time. The rationale behind filtering out words with a frequency of less than three is to improve model accuracy. Including words that occur very infrequently can negatively impact model performance. For instance, if a word is present in the training data but does not appear in the test dataset, the model might not generalize well and could lead to inaccurate predictions. Therefore, by removing such infrequent words, the main aim is to enhance the model's ability to make more reliable predictions.

The next part is training the model and converting the description column and genre column into numerical format. For converting the genre in numerical, we are creating the matrix with the size of the data frame shape and a number of distinct genres present. The next step is creating a matrix with 1s and 0s according to the lookup table. Using text vectorization, convert text into numbers. The neural network model is constructed using tensorflow and keras. In text vectorization, the vocabulary size is set to 15000 words. The model has 64 dense layers where it has relu activation and 28 layers using softmax activation layer for multi-class classification of genres. The embedding layer is used for mapping vocabulary tokens. Adam optimizer is used for model training.

The model is evaluated on test data and the model is saved using Keras. Here pickle is used for storing a lookup table for predicting the genres. Once we pass the input file to the model it predicts the top 3 genres suitable for that description.

# Search Design

Initially, before the index, we need to clean the dataset. Checking for null values, removing punctuations, symbols, and numbers as they do not have any effect on searching. Here lemmetization is used for indexing. Lemmetization is used to get the standardised words from their root words. This can help in improving our search analysis. The sentence passed from the description column is tokenized. Each word is tokenized and the sentence is split Wordnet Lemmmatizer is used for performing lemmatization. WordNet lemmatizer is used for lemmatization. wordnet.verb is used to get the part of speech of the word as a verb.

Once the indexing is done using lemmatization, bm250kapi is used for searching and bm25 can also be used to do indexing based on the query which is provided. The tokenized text is passed into bm250kapi. This API is the advanced version of tf-idf algorithm where it ranks the documents based on the frequency of the words, using IDF it calculates the importance of the terms.
